{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cf6e845",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/11 15:35:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "import datetime\n",
    "import optuna \n",
    "\n",
    "import pandas as pd\n",
    "import pprint\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import (\n",
    "    sum, col, collect_list,\n",
    "    when, explode\n",
    ")\n",
    "\n",
    "from pyspark.ml.evaluation import RankingEvaluator \n",
    "from pyspark.ml.recommendation import ALS \n",
    "\n",
    "\n",
    "spark = SparkSession.builder.master('spark://cm010:28844').getOrCreate()\n",
    "# spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "# spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "# 16 cores times 8 cpus = 128 partitions * 2 = 384 partitions\n",
    "# spark.conf.set(\"spark.sql.shuffle.partitions\", 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81cd43ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_utiltiy_matrix(interactions, tracks):\n",
    "    interactions.createOrReplaceTempView('interactions')\n",
    "    tracks.createOrReplaceTempView('tracks')\n",
    "\n",
    "    listens_per_user_track = spark.sql(\n",
    "        \"\"\"\n",
    "        SELECT user_id,universal_id,sum(num_listens) as num_listens\n",
    "        FROM interactions\n",
    "        LEFT JOIN tracks\n",
    "        ON tracks.recording_msid=interactions.recording_msid\n",
    "        GROUP BY user_id,universal_id\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    listens_per_user = listens_per_user_track.select(\n",
    "        listens_per_user_track.user_id, listens_per_user_track.num_listens\n",
    "    ).groupBy('user_id').agg(\n",
    "        sum(listens_per_user_track.num_listens).alias('total_listens')\n",
    "    )\n",
    "\n",
    "    listens_per_user = listens_per_user.withColumn(\n",
    "        'use_for_fit',\n",
    "        when(\n",
    "            listens_per_user.total_listens >= 500, True\n",
    "        ).otherwise(\n",
    "            False\n",
    "        )\n",
    "    )\n",
    "\n",
    "    normed_listens_per_user_track = listens_per_user_track.join(listens_per_user, how='left', on='user_id')\n",
    "    normed_listens_per_user_track = normed_listens_per_user_track.withColumn(\n",
    "        \"prop_listens\",\n",
    "        col(\"num_listens\")/col(\"total_listens\")\n",
    "    ).select(\n",
    "        ['user_id', 'universal_id', 'prop_listens', 'use_for_fit']\n",
    "    ).orderBy(\n",
    "        col('user_id').asc(),\n",
    "        col('prop_listens').desc()\n",
    "    )\n",
    "\n",
    "    return normed_listens_per_user_track\n",
    "\n",
    "def calc_performance_metrics_als(predicted, actual, calc_ndcg: bool = False):\n",
    "    actual_compressed = actual.groupBy(\n",
    "        'user_id'\n",
    "    ).agg(\n",
    "        collect_list(col('universal_id').astype('double')).alias('universal_id'),\n",
    "        collect_list(col('prop_listens').astype('double')).alias('prop_listens')\n",
    "    )\n",
    "\n",
    "    predicted_compressed = predicted.withColumn(\n",
    "        \"recommendations\", explode(col(\"recommendations\"))\n",
    "    ).select(\"user_id\", \"recommendations.universal_id\", \"recommendations.rating\")\n",
    "\n",
    "    predicted_compressed = predicted_compressed.withColumn(\n",
    "        \"rn\", F.row_number().over(Window.partitionBy(\"user_id\").orderBy(F.col(\"rating\").desc()))\n",
    "    ).groupBy(\"user_id\").agg(F.collect_list(F.col(\"universal_id\")).astype('array<double>').alias(\"predicted_universal_id\"))\n",
    "\n",
    "    results = actual_compressed.join(\n",
    "        predicted_compressed,\n",
    "        how='inner',\n",
    "        on='user_id'\n",
    "    )\n",
    "    \n",
    "    mapAtK = RankingEvaluator(\n",
    "        predictionCol='predicted_universal_id',\n",
    "        labelCol='universal_id',\n",
    "        metricName='meanAveragePrecisionAtK',\n",
    "        k=100\n",
    "    )\n",
    "    \n",
    "    if calc_ndcg:\n",
    "        ndcgAtK = RankingEvaluator(\n",
    "            predictionCol='predicted_universal_id',\n",
    "            labelCol='universal_id',\n",
    "            metricName='ndcgAtK',\n",
    "            k=100\n",
    "        )\n",
    "        return (mapAtK.evaluate(results), ndcgAtK.evaluate(results))\n",
    "    \n",
    "    return mapAtK.evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d889aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "interactions_train = spark.read.parquet('interactions_split_train.parquet')\n",
    "interactions_val = spark.read.parquet('interactions_split_val.parquet')\n",
    "interactions_test = spark.read.parquet(\"/scratch/work/courses/DSGA1004-2021/listenbrainz/interactions_test.parquet\")\n",
    "\n",
    "tracks_train = spark.read.parquet('tracks_train.parquet')\n",
    "tracks_test = spark.read.parquet('tracks_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7ec926",
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_mat_train = gen_utiltiy_matrix(interactions_train, tracks_train)\n",
    "utility_mat_val = gen_utiltiy_matrix(interactions_val, tracks_train)\n",
    "utility_mat_train = utility_mat_train.filter(\n",
    "    utility_mat_train.use_for_fit\n",
    ")\n",
    "utility_mat_test = gen_utiltiy_matrix(interactions_test, tracks_test)\n",
    "\n",
    "# utility_mat_train = spark.read.parquet('utility_mat_train_sample.parquet')\n",
    "# utility_mat_train.sample(withReplacement=True, fraction=0.1, seed=69).write.parquet('utility_mat_train_sample.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6050a2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-10 13:32:27,469]\u001b[0m Using an existing study with name 'als_search' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2023-05-10 13:45:26,175]\u001b[0m Trial 41 finished with value: 0.07904716251052674 and parameters: {'rank': 9, 'alpha': 139.96114660196292, 'regParam': 0.15459224428752139}. Best is trial 35 with value: 0.1371739926525655.\u001b[0m\n",
      "\u001b[32m[I 2023-05-10 14:39:34,088]\u001b[0m Trial 42 finished with value: 0.13690620298938463 and parameters: {'rank': 50, 'alpha': 437.6193598547283, 'regParam': 0.02554729728351489}. Best is trial 35 with value: 0.1371739926525655.\u001b[0m\n",
      "\u001b[32m[I 2023-05-10 14:47:39,143]\u001b[0m Trial 43 finished with value: 0.03071384935010459 and parameters: {'rank': 1, 'alpha': 453.4580027901268, 'regParam': 0.07685896205562705}. Best is trial 35 with value: 0.1371739926525655.\u001b[0m\n",
      "\u001b[32m[I 2023-05-10 15:35:30,030]\u001b[0m Trial 44 finished with value: 0.13296560561570792 and parameters: {'rank': 48, 'alpha': 503.00581166072936, 'regParam': 0.02464430608585112}. Best is trial 35 with value: 0.1371739926525655.\u001b[0m\n",
      "\u001b[32m[I 2023-05-10 16:25:21,510]\u001b[0m Trial 45 finished with value: 0.13473774292335297 and parameters: {'rank': 50, 'alpha': 30.816061194273978, 'regParam': 0.025805017934644563}. Best is trial 35 with value: 0.1371739926525655.\u001b[0m\n",
      "\u001b[32m[I 2023-05-10 17:10:02,378]\u001b[0m Trial 46 finished with value: 0.1359390854993289 and parameters: {'rank': 46, 'alpha': 94.19523166929854, 'regParam': 0.011669489350219477}. Best is trial 35 with value: 0.1371739926525655.\u001b[0m\n",
      "\u001b[32m[I 2023-05-10 17:59:35,050]\u001b[0m Trial 47 finished with value: 0.09851460788900938 and parameters: {'rank': 43, 'alpha': 390.92271383156816, 'regParam': 0.2165861187989354}. Best is trial 35 with value: 0.1371739926525655.\u001b[0m\n",
      "\u001b[32m[I 2023-05-10 18:49:36,457]\u001b[0m Trial 48 finished with value: 0.1266867220337707 and parameters: {'rank': 48, 'alpha': 6.741249461267146, 'regParam': 0.06952044235038894}. Best is trial 35 with value: 0.1371739926525655.\u001b[0m\n",
      "\u001b[32m[I 2023-05-10 19:08:23,639]\u001b[0m Trial 49 finished with value: 0.12199344229995648 and parameters: {'rank': 24, 'alpha': 983.4815599567519, 'regParam': 0.02262605849015924}. Best is trial 35 with value: 0.1371739926525655.\u001b[0m\n",
      "\u001b[32m[I 2023-05-10 19:32:23,905]\u001b[0m Trial 50 finished with value: 0.12656792417367668 and parameters: {'rank': 32, 'alpha': 179.26569001156568, 'regParam': 0.010542558923583436}. Best is trial 35 with value: 0.1371739926525655.\u001b[0m\n",
      "[Stage 6773:>                                                     (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/10 19:36:46 ERROR TaskSchedulerImpl: Lost executor 0 on 10.32.34.127: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/05/10 19:36:46 WARN TaskSetManager: Lost task 6.0 in stage 6773.0 (TID 49404) (10.32.34.127 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/05/10 19:36:46 WARN TaskSetManager: Lost task 9.0 in stage 6773.0 (TID 49407) (10.32.34.127 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/05/10 19:36:46 WARN TaskSetManager: Lost task 0.0 in stage 6773.0 (TID 49398) (10.32.34.127 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/05/10 19:36:46 WARN TaskSetManager: Lost task 3.0 in stage 6773.0 (TID 49401) (10.32.34.127 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/05/10 19:36:46 WARN TaskSetManager: Lost task 5.0 in stage 6773.0 (TID 49403) (10.32.34.127 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/05/10 19:36:46 WARN TaskSetManager: Lost task 8.0 in stage 6773.0 (TID 49406) (10.32.34.127 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/05/10 19:36:46 WARN TaskSetManager: Lost task 2.0 in stage 6773.0 (TID 49400) (10.32.34.127 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/05/10 19:36:46 WARN TaskSetManager: Lost task 1.0 in stage 6773.0 (TID 49399) (10.32.34.127 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/05/10 19:36:46 WARN TaskSetManager: Lost task 4.0 in stage 6773.0 (TID 49402) (10.32.34.127 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/05/10 19:36:46 WARN TaskSetManager: Lost task 7.0 in stage 6773.0 (TID 49405) (10.32.34.127 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3648_2 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3647_3 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_95 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_196 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3643_8 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_56 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_191 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_108 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3642_7 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3647_0 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_9 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_174 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_183 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3660_6 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_186 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3660_2 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3643_2 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_154 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_178 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_133 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_117 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_165 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3648_7 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3642_9 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_166 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_1 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3660_7 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_61 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_144 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_145 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_6 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_119 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_143 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3643_1 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3643_7 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3643_4 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_132 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3648_1 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3647_9 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_26 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_101 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_68 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_33 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3643_3 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_38 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_158 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3642_4 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_75 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_134 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3660_9 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3642_3 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3660_4 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3642_1 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_161 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_155 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_8 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3642_2 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3660_8 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3647_2 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_25 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_58 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_83 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_159 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3648_3 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3643_9 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_163 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3660_3 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_32 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3648_4 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3643_0 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_147 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3647_1 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3648_9 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3642_0 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_66 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_46 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_5 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_7 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_50 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_39 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_138 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_110 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_76 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3643_5 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_31 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_197 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_193 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3647_6 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3648_6 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3647_8 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_112 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_3 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_72 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3648_8 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_94 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3660_1 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_151 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_43 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3642_5 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_63 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3647_5 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_137 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_105 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_126 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_40 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_71 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_118 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3647_7 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3643_6 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_130 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3642_8 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_44 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_89 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_192 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3660_0 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_136 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3660_5 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_148 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_2 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_160 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_29 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3642_6 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3648_0 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3647_4 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_49 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3639_150 !\n",
      "23/05/10 19:36:46 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_3648_5 !\n",
      "23/05/10 19:36:46 WARN TaskSetManager: Lost task 8.1 in stage 6773.0 (TID 49412) (10.32.33.167 executor 3): FetchFailed(null, shuffleId=432, mapIndex=-1, mapId=-1, reduceId=8, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 432 partition 8\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1701)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10(MapOutputTracker.scala:1648)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10$adapted(MapOutputTracker.scala:1647)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1647)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1290)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1252)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.CoGroupedRDD.$anonfun$compute$2(CoGroupedRDD.scala:140)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:985)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:984)\n",
      "\tat org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:136)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      ")\n",
      "23/05/10 19:36:46 WARN TaskSetManager: Lost task 2.1 in stage 6773.0 (TID 49411) (10.32.33.30 executor 6): FetchFailed(null, shuffleId=432, mapIndex=-1, mapId=-1, reduceId=2, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 432 partition 2\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1701)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10(MapOutputTracker.scala:1648)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10$adapted(MapOutputTracker.scala:1647)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1647)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1290)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1252)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.CoGroupedRDD.$anonfun$compute$2(CoGroupedRDD.scala:140)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:985)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:984)\n",
      "\tat org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:136)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      ")\n",
      "23/05/10 19:36:46 WARN TaskSetManager: Lost task 3.1 in stage 6773.0 (TID 49414) (10.32.33.168 executor 7): FetchFailed(null, shuffleId=432, mapIndex=-1, mapId=-1, reduceId=3, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 432 partition 3\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1701)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10(MapOutputTracker.scala:1648)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10$adapted(MapOutputTracker.scala:1647)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1647)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1290)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1252)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.CoGroupedRDD.$anonfun$compute$2(CoGroupedRDD.scala:140)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:985)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:984)\n",
      "\tat org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:136)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      ")\n",
      "23/05/10 19:36:46 WARN TaskSetManager: Lost task 5.1 in stage 6773.0 (TID 49413) (10.32.33.29 executor 2): FetchFailed(null, shuffleId=432, mapIndex=-1, mapId=-1, reduceId=5, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 432 partition 5\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1701)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10(MapOutputTracker.scala:1648)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10$adapted(MapOutputTracker.scala:1647)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1647)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1290)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1252)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.CoGroupedRDD.$anonfun$compute$2(CoGroupedRDD.scala:140)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:985)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:984)\n",
      "\tat org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:136)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      ")\n",
      "23/05/10 19:36:46 WARN TaskSetManager: Lost task 1.1 in stage 6773.0 (TID 49410) (10.32.33.61 executor 5): FetchFailed(null, shuffleId=432, mapIndex=-1, mapId=-1, reduceId=1, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 432 partition 1\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1701)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10(MapOutputTracker.scala:1648)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10$adapted(MapOutputTracker.scala:1647)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1647)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1290)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1252)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.CoGroupedRDD.$anonfun$compute$2(CoGroupedRDD.scala:140)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:985)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:984)\n",
      "\tat org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:136)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      ")\n",
      "23/05/10 19:36:46 WARN TaskSetManager: Lost task 6.1 in stage 6773.0 (TID 49417) (10.32.33.61 executor 5): FetchFailed(null, shuffleId=432, mapIndex=-1, mapId=-1, reduceId=6, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 432 partition 6\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1701)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10(MapOutputTracker.scala:1648)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10$adapted(MapOutputTracker.scala:1647)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1647)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1290)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1252)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.CoGroupedRDD.$anonfun$compute$2(CoGroupedRDD.scala:140)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:985)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:984)\n",
      "\tat org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:136)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      ")\n",
      "23/05/10 19:36:46 WARN TaskSetManager: Lost task 9.1 in stage 6773.0 (TID 49416) (10.32.34.126 executor 1): FetchFailed(null, shuffleId=432, mapIndex=-1, mapId=-1, reduceId=9, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 432 partition 9\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1701)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10(MapOutputTracker.scala:1648)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10$adapted(MapOutputTracker.scala:1647)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1647)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1290)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1252)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.CoGroupedRDD.$anonfun$compute$2(CoGroupedRDD.scala:140)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:985)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:984)\n",
      "\tat org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:136)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      ")\n",
      "23/05/10 19:36:46 WARN TaskSetManager: Lost task 4.1 in stage 6773.0 (TID 49409) (10.32.34.126 executor 1): FetchFailed(null, shuffleId=432, mapIndex=-1, mapId=-1, reduceId=4, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 432 partition 4\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1701)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10(MapOutputTracker.scala:1648)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10$adapted(MapOutputTracker.scala:1647)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1647)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1290)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1252)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.CoGroupedRDD.$anonfun$compute$2(CoGroupedRDD.scala:140)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:985)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:984)\n",
      "\tat org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:136)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      ")\n",
      "23/05/10 19:36:46 WARN TaskSetManager: Lost task 7.1 in stage 6773.0 (TID 49408) (10.32.34.128 executor 4): FetchFailed(null, shuffleId=432, mapIndex=-1, mapId=-1, reduceId=7, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 432 partition 7\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1701)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10(MapOutputTracker.scala:1648)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10$adapted(MapOutputTracker.scala:1647)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1647)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1290)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1252)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.CoGroupedRDD.$anonfun$compute$2(CoGroupedRDD.scala:140)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:985)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:984)\n",
      "\tat org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:136)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      ")\n",
      "23/05/10 19:36:46 WARN TaskSetManager: Lost task 0.1 in stage 6773.0 (TID 49415) (10.32.34.128 executor 4): FetchFailed(null, shuffleId=432, mapIndex=-1, mapId=-1, reduceId=0, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 432 partition 0\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1701)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10(MapOutputTracker.scala:1648)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10$adapted(MapOutputTracker.scala:1647)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1647)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1290)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1252)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.CoGroupedRDD.$anonfun$compute$2(CoGroupedRDD.scala:140)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:985)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:984)\n",
      "\tat org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:136)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6848:>                                                     (0 + 10) / 10]\r"
     ]
    }
   ],
   "source": [
    "# Handle different parameters \n",
    "SEED = 69\n",
    "TRIALS = 500\n",
    "MAXITER = 10\n",
    "\n",
    "# Utilize random search to find optimal hyperparameters \n",
    "# NOTE: alpha is not tuned \n",
    "\n",
    "training_results = []\n",
    "start = time.perf_counter()\n",
    "\n",
    "def objective(trial):\n",
    "    rank = trial.suggest_int('rank', 1, 50)\n",
    "    alpha = trial.suggest_float('alpha', 1e-3, 1e3, log=True)\n",
    "    regParam = trial.suggest_float('regParam', 1e-2, 1e5, log=True)\n",
    "    \n",
    "    als = ALS(\n",
    "        maxIter=MAXITER, \n",
    "        alpha=alpha,\n",
    "        regParam=regParam,\n",
    "        implicitPrefs=True,\n",
    "        nonnegative = True, \n",
    "        rank=rank, \n",
    "        seed=SEED, \n",
    "        userCol='user_id',\n",
    "        itemCol='universal_id',\n",
    "        ratingCol='prop_listens',\n",
    "        coldStartStrategy='drop'\n",
    "    ) \n",
    "    \n",
    "    # Calculate MAP\n",
    "    model = als.fit(utility_mat_train)\n",
    "    \n",
    "#     predictions_train =  model.recommendForUserSubset(utility_mat_train.select('user_id').distinct(), 100)\n",
    "    predictions_val =  model.recommendForUserSubset(utility_mat_val.select('user_id').distinct(), 100)\n",
    "    \n",
    "#     map_train, ndcg_train = calc_performance_metrics_als(predictions_train, utility_mat_train)\n",
    "    map_val = calc_performance_metrics_als(predictions_val, utility_mat_val)\n",
    "    \n",
    "    return map_val\n",
    "    \n",
    "study = optuna.create_study(\n",
    "    study_name='als_search',\n",
    "    direction='maximize', \n",
    "    sampler=optuna.samplers.TPESampler(seed=SEED), \n",
    "    storage='sqlite:///optuna.db', \n",
    "    load_if_exists=True\n",
    ")\n",
    "study.optimize(objective, n_trials=TRIALS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f282628d",
   "metadata": {},
   "source": [
    "## Train Using the Best Params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78afcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(\n",
    "    study_name='als_search',\n",
    "    direction='maximize', \n",
    "    sampler=optuna.samplers.TPESampler(seed=69), \n",
    "    storage='sqlite:///optuna.db', \n",
    "    load_if_exists=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c92504",
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(\n",
    "    maxIter=10, \n",
    "    alpha=study.best_params['alpha'],\n",
    "    regParam=study.best_params['regParam'],\n",
    "    implicitPrefs=True,\n",
    "    nonnegative = True, \n",
    "    rank=study.best_params['rank'], \n",
    "    seed=69, \n",
    "    userCol='user_id',\n",
    "    itemCol='universal_id',\n",
    "    ratingCol='prop_listens',\n",
    "    coldStartStrategy='drop'\n",
    ") \n",
    "\n",
    "# Calculate MAP\n",
    "model = als.fit(utility_mat_train)\n",
    "\n",
    "predictions_train =  model.recommendForUserSubset(utility_mat_train.select('user_id').distinct(), 100)\n",
    "predictions_val =  model.recommendForUserSubset(utility_mat_val.select('user_id').distinct(), 100)\n",
    "predictions_test =  model.recommendForUserSubset(utility_mat_test.select('user_id').distinct(), 100)\n",
    "\n",
    "%time map_train, ndcg_train = calc_performance_metrics_als(predictions_train, utility_mat_train, calc_ndcg=True)\n",
    "%time map_val, ndcg_val = calc_performance_metrics_als(predictions_val, utility_mat_val, calc_ndcg=True)\n",
    "%time map_test, ndcg_test = calc_performance_metrics_als(predictions_test, utility_mat_test, calc_ndcg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d352ccd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab46e17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_train, ndcg_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cc8db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_val, ndcg_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1661cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_test, ndcg_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
