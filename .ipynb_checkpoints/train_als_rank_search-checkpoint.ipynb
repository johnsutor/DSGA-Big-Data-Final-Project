{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cf6e845",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/05 09:49:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time \n",
    "import datetime\n",
    "from itertools import product \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import (\n",
    "    max, avg, sum, count, countDistinct,\n",
    "    percentile_approx, col, asc, desc, collect_list,\n",
    "    lit, rand, when, to_date, collect_set, explode\n",
    ")\n",
    "from pyspark.ml.evaluation import RankingEvaluator, RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS \n",
    "\n",
    "from scipy.stats import loguniform, randint\n",
    "\n",
    "spark = SparkSession.builder.master('spark://cm001:61086').getOrCreate()\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "# 16 cores times 8 cpus = 128 partitions * 2 = 384 partitions\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81cd43ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_utiltiy_matrix(interactions, tracks):\n",
    "    interactions.createOrReplaceTempView('interactions')\n",
    "    tracks.createOrReplaceTempView('tracks')\n",
    "\n",
    "    listens_per_user_track = spark.sql(\n",
    "        \"\"\"\n",
    "        SELECT user_id,universal_id,sum(num_listens) as num_listens\n",
    "        FROM interactions\n",
    "        LEFT JOIN tracks\n",
    "        ON tracks.recording_msid=interactions.recording_msid\n",
    "        GROUP BY user_id,universal_id\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    listens_per_user = listens_per_user_track.select(\n",
    "        listens_per_user_track.user_id, listens_per_user_track.num_listens\n",
    "    ).groupBy('user_id').agg(\n",
    "        sum(listens_per_user_track.num_listens).alias('total_listens')\n",
    "    )\n",
    "\n",
    "    listens_per_user = listens_per_user.withColumn(\n",
    "        'use_for_fit',\n",
    "        when(\n",
    "            listens_per_user.total_listens >= 500, True\n",
    "        ).otherwise(\n",
    "            False\n",
    "        )\n",
    "    )\n",
    "\n",
    "    normed_listens_per_user_track = listens_per_user_track.join(listens_per_user, how='left', on='user_id')\n",
    "    normed_listens_per_user_track = normed_listens_per_user_track.withColumn(\n",
    "        \"prop_listens\",\n",
    "        col(\"num_listens\")/col(\"total_listens\")\n",
    "    ).select(\n",
    "        ['user_id', 'universal_id', 'prop_listens', 'use_for_fit']\n",
    "    ).orderBy(\n",
    "        col('user_id').asc(),\n",
    "        col('prop_listens').desc()\n",
    "    )\n",
    "\n",
    "    return normed_listens_per_user_track\n",
    "\n",
    "def calc_performance_metrics_als(predicted, actual, calc_ndcg: bool = False):\n",
    "    actual_compressed = actual.groupBy(\n",
    "        'user_id'\n",
    "    ).agg(\n",
    "        collect_list(col('universal_id').astype('double')).alias('universal_id'),\n",
    "        collect_list(col('prop_listens').astype('double')).alias('prop_listens')\n",
    "    )\n",
    "\n",
    "    predicted_compressed = predicted.withColumn(\n",
    "        \"recommendations\", explode(col(\"recommendations\"))\n",
    "    ).select(\"user_id\", \"recommendations.universal_id\", \"recommendations.rating\")\n",
    "\n",
    "    predicted_compressed = predicted_compressed.withColumn(\n",
    "        \"rn\", F.row_number().over(Window.partitionBy(\"user_id\").orderBy(F.col(\"rating\").desc()))\n",
    "    ).groupBy(\"user_id\").agg(F.collect_list(F.col(\"universal_id\")).astype('array<double>').alias(\"predicted_universal_id\"))\n",
    "\n",
    "    results = actual_compressed.join(\n",
    "        predicted_compressed,\n",
    "        how='inner',\n",
    "        on='user_id'\n",
    "    )\n",
    "    \n",
    "    mapAtK = RankingEvaluator(\n",
    "        predictionCol='predicted_universal_id',\n",
    "        labelCol='universal_id',\n",
    "        metricName='meanAveragePrecisionAtK',\n",
    "        k=100\n",
    "    )\n",
    "    \n",
    "    if calc_ndcg:\n",
    "        ndcgAtK = RankingEvaluator(\n",
    "            predictionCol='predicted_universal_id',\n",
    "            labelCol='universal_id',\n",
    "            metricName='ndcgAtK',\n",
    "            k=100\n",
    "        )\n",
    "        return (mapAtK.evaluate(results), ndcgAtK.evaluate(results))\n",
    "    \n",
    "    return mapAtK.evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d889aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "interactions_train = spark.read.parquet('interactions_split_train.parquet')\n",
    "interactions_val = spark.read.parquet('interactions_split_val.parquet')\n",
    "# interactions_test = spark.read.parquet(\"/scratch/work/courses/DSGA1004-2021/listenbrainz/interactions_test.parquet\")\n",
    "\n",
    "tracks_train = spark.read.parquet('tracks_train.parquet')\n",
    "# tracks_test = spark.read.parquet('tracks_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef7ec926",
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_mat_train = gen_utiltiy_matrix(interactions_train, tracks_train)\n",
    "utility_mat_val = gen_utiltiy_matrix(interactions_val, tracks_train)\n",
    "# utility_mat_test = gen_utiltiy_matrix(interactions_test, tracks_test)\n",
    "# utility_mat_train = spark.read.parquet('utility_mat_train_sample_nr.parquet')\n",
    "\n",
    "utility_mat_train = utility_mat_train.filter(\n",
    "    utility_mat_train.use_for_fit\n",
    ")\n",
    "# utility_mat_train.sample(withReplacement=False, fraction=0.1, seed=69).write.parquet('utility_mat_train_sample_nr.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6050a2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'elapsed': '0:53:26.996693',\n",
      " 'map_val': 0.12051327034679808,\n",
      " 'maxIter': 10,\n",
      " 'rank': 50,\n",
      " 'seed': 69,\n",
      " 'trial': 51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 717:>                                                      (0 + 10) / 10]\r"
     ]
    }
   ],
   "source": [
    "# Handle different parameters \n",
    "SEED = 69\n",
    "TRIALS = 100\n",
    "MAXITER = 10\n",
    "\n",
    "# Utilize random search to find optimal hyperparameters \n",
    "# NOTE: alpha is not tuned \n",
    "\n",
    "training_results = []\n",
    "start = time.perf_counter()\n",
    "\n",
    "for r in range(10, 100, 10):\n",
    "    als = ALS(\n",
    "        maxIter=MAXITER, \n",
    "        implicitPrefs=True,\n",
    "        nonnegative = True, \n",
    "        rank=r, \n",
    "        seed=SEED, \n",
    "        userCol='user_id',\n",
    "        itemCol='universal_id',\n",
    "        ratingCol='prop_listens',\n",
    "        coldStartStrategy='drop'\n",
    "    ) \n",
    "    \n",
    "    # Calculate MAP\n",
    "    model = als.fit(utility_mat_train)\n",
    "    \n",
    "#     predictions_train =  model.recommendForUserSubset(utility_mat_train.select('user_id').distinct(), 100)\n",
    "    predictions_val =  model.recommendForUserSubset(utility_mat_val.select('user_id').distinct(), 100)\n",
    "    \n",
    "#     map_train, ndcg_train = calc_performance_metrics_als(predictions_train, utility_mat_train)\n",
    "    map_val = calc_performance_metrics_als(predictions_val, utility_mat_val)\n",
    "\n",
    "    elapsed = datetime.timedelta(seconds=time.perf_counter() - start)\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    training_results.append({\n",
    "        \"maxIter\": MAXITER, \n",
    "        \"seed\": SEED,\n",
    "        \"elapsed\": str(elapsed),\n",
    "        \"rank\": r, \n",
    "        \"map_val\": map_val,\n",
    "        \"trial\": r + 1,\n",
    "#         \"map_train\": map_train,\n",
    "#         \"ndcg_train\": ndcg_train,\n",
    "#         \"ndcg_val\": ndcg_val,\n",
    "    })\n",
    "    \n",
    "    pprint.pprint(training_results[-1], width=1)\n",
    "    pd.DataFrame(training_results).to_csv('./results_runs/als_results_rank_step_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a23712b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
